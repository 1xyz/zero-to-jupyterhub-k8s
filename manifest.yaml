#
# We are using a GCE disk as persistent storage for the Hub
# and eventually, each individual user node.
#
# An administrator with access must provision these disks with a reasonable amount of space
# before hand. Please note that with the K8s deployment, we are no longer using a shared NFS
# and instead each user will have his or her own disk.
#
# Find the region for your cluster beforehand using:
#
#     gcloud container clusters list
#
# Provision your disk like so:
#
#     gcloud compute disks create hub-workdir-01 --size 1GiB
#
# Please make sure to delete your disk after you are done using it as you will be charged.
#
#     gcloud compute disks delete hub-workdir-01

kind: ConfigMap
apiVersion: v1
metadata:
  name: hub-config
data:
  # Used to authenticate the culler to the hub. This string was generated with `openssl rand -hex 32`.
  auth.jhub-token.cull: 13fdff1305cd883e49223908186a63294922dadb59b5d1122473041f160c4b03
  # Used to authenticate the hub to the proxy. This string was generated with `pwgen 64`.
  # Please generate a new one for your own deployment!
  auth.jhub-token.proxy: Kev4Shai9phai0Eez2aiyaefaepheutei3baehaiseipheef1Ah2cah4xeaquohr
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: allan-workdir-01
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  gcePersistentDisk:
    pdName: allan-workdir-01
    fsType: ext4
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: data8-workdir
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  name: proxy-api
spec:
  selector:
    name: proxy-pod
  ports:
    - protocol: TCP
      port: 8001
      targetPort: 8001
---
apiVersion: v1
kind: Service
metadata:
  name: proxy-public
spec:
  type: LoadBalancer
  selector:
    name: proxy-pod
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8000
---
apiVersion: v1
kind: Service
metadata:
  name: hub
spec:
  selector:
    name: hub-pod
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 8081
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: hub-deployment
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: hub-pod
    spec:
      volumes:
      - name: data8-workdir-volume
        persistentVolumeClaim:
          claimName: data8-workdir
      containers:
      - name: hub-container
        image: data8/jupyterhub-k8s-hub:separate-proxy
        imagePullPolicy: Always
        volumeMounts:
          - mountPath: /srv/jupyterhub
            name: data8-workdir-volume
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CULL_JHUB_TOKEN
          valueFrom:
            configMapKeyRef:
              name: hub-config
              key: auth.jhub-token.cull
        - name: CONFIGPROXY_AUTH_TOKEN
          valueFrom:
            configMapKeyRef:
              name: hub-config
              key: auth.jhub-token.proxy
        ports:
          - containerPort: 8081
            name: hub
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: proxy-deployment
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: proxy-pod
    spec:
      containers:
      - name: proxy-container
        image: yuvipanda/nchp:v1
        env:
          - name: CONFIGPROXY_AUTH_TOKEN
            valueFrom:
              configMapKeyRef:
                name: hub-config
                key: auth.jhub-token.proxy
        ports:
          - containerPort: 8000
            name: proxy-public
          - containerPort: 8001
            name: api
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: cull-deployment
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: cull-pod
    spec:
      containers:
        - name: cull-container
          image: data8/jupyterhub-k8s-cull:master
          command:
            - /bin/sh
            - -c
          args: ['python /srv/cull/cull_idle_servers.py --timeout=3600 --cull_every=600 --url=http://${HUB_SERVICE_HOST}:${HUB_SERVICE_PORT}/hub']
          env:
          - name: JPY_API_TOKEN
            valueFrom:
              configMapKeyRef:
                name: cull-config
                key: auth.jhub-token.cull
