#
# We are using a GCE disk as persistent storage for the Hub
# and for each individual user node.
#
# A volume for the hub is automatically provisioned b/c its pvc
# is using a storage class. There is no need to manually provision
# any disks on the cluster.
# 
# When a single user pod is spun up, a corresponding pvc is 
# dynamically provisioned by kubespawner. If a pvc for that user
# already exists, the pod mounts to the existing pvc.
# 

kind: StorageClass
apiVersion: storage.k8s.io/v1beta1
metadata:
  name: gce-standard-storage
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
  zone: us-central1-a
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: cull-config
data:
  # Used to authenticate the culler to the hub. This string was generated with `openssl rand -hex 32`.
  auth.jhub-token.cull: 13fdff1305cd883e49223908186a63294922dadb59b5d1122473041f160c4b03
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: data8-workdir
  annotations:
    volume.beta.kubernetes.io/storage-class: gce-standard-storage
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  name: hub-proxy
spec:
  type: LoadBalancer
  selector:
    name: hub-proxy-pod
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8000
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: hub-proxy-deployment
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: hub-proxy-pod
    spec:
      volumes:
      - name: data8-workdir-volume
        persistentVolumeClaim:
          claimName: data8-workdir
      containers:
      - name: hub-proxy-container
        image: data8/jupyterhub-k8s-hub:master
        imagePullPolicy: Always
        volumeMounts:
          - mountPath: /srv/jupyterhub
            name: data8-workdir-volume
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CULL_JHUB_TOKEN
          valueFrom:
            configMapKeyRef:
              name: cull-config
              key: auth.jhub-token.cull
        ports:
          - containerPort: 8000
            name: hub-proxy-port
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: cull-deployment
spec:
  replicas: 1
  template:
    metadata:
      labels:
        name: cull-pod
    spec:
      containers:
        - name: cull-container
          image: data8/jupyterhub-k8s-cull:master
          command:
            - /bin/sh
            - -c
          args: ['python /srv/cull/cull_idle_servers.py --timeout=3600 --cull_every=600 --url=http://${HUB_PROXY_SERVICE_HOST}:${HUB_PROXY_SERVICE_PORT}/hub']
          env:
          - name: JPY_API_TOKEN
            valueFrom:
              configMapKeyRef:
                name: cull-config
                key: auth.jhub-token.cull
